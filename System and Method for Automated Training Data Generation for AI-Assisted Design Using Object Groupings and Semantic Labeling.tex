\documentclass{article}

\usepackage{graphicx}
\usepackage{hyperref}

\title{System and Method for Automated Training Data Generation for AI-Assisted Design Using Object Groupings and Semantic Labeling}

\begin{document}


\begin{abstract}
   %This invention discloses a novel system and method for automating the creation of training data for artificial intelligence (AI) models utilized in design applications, particularly within the realm of computer-aided design (CAD). The system leverages the inherent structure of CAD models, specifically object groupings based on design principles and hierarchical relationships, to generate a rich and diverse dataset for training AI. This dataset is generated by systematically varying visual attributes of the CAD model, including color, material, finish (CMF), camera perspective, lighting conditions, and background, while meticulously preserving the core design form. This form isolation is critical to enabling the AI model to learn and generalize the underlying design intent without overfitting to specific stylistic choices or visual artifacts. Semantic labels, automatically derived from the object groupings and applied variations, accompany each generated image. The resulting dataset of images and semantic labels is then used to fine-tune a pre-trained text-to-image AI model, empowering designers to generate novel renderings through natural language prompts. These prompts can specify adjustments to CMF properties and other visual attributes, offering unprecedented control and flexibility while ensuring the integrity of the original design form. The system further incorporates advanced features like automated file management, location-based background integration, cost and environmental impact analysis, automated RFQ generation, design callouts, character continuity management, dynamic UI sliders, patent drawing generation, design analysis feedback, client annotation tools, and image security features, significantly streamlining the design workflow and enhancing the collaborative design process.

   This invention discloses a novel system and method for automating the creation of training data for artificial intelligence (AI) models utilized in design applications, particularly within the realm of computer-aided design (CAD). The system leverages the inherent structure of CAD models, specifically object groupings based on design principles and hierarchical relationships, to generate a rich and diverse dataset for training AI. This dataset is generated by systematically varying visual attributes of the CAD model, including color, material, finish (CMF), camera perspective, lighting conditions, and background, while meticulously preserving the core design form. This form isolation is critical to enabling the AI model to learn and generalize the underlying design intent without overfitting to specific stylistic choices or visual artifacts. Semantic labels, automatically derived from the object groupings and applied variations, accompany each generated image. The resulting dataset of images and semantic labels is then used to fine-tune a pre-trained text-to-image AI model, empowering designers to generate novel renderings through natural language prompts. These prompts can specify adjustments to CMF properties and other visual attributes, offering unprecedented control and flexibility while ensuring the integrity of the original design form. The system further incorporates advanced features like automated file management, location-based background integration, cost and environmental impact analysis, automated RFQ generation, design callouts, character continuity management, dynamic UI sliders, patent drawing generation, design analysis feedback, client annotation tools, and image security features, significantly streamlining the design workflow and enhancing the collaborative design process.




\end{abstract}

\section{Background of the Invention}

\subsection{Field of the Invention}

%This invention lies at the confluence of artificial intelligence (AI), computer-aided design (CAD), computer graphics, machine learning, and design theory, specifically addressing the automated creation of training datasets for AI-driven design applications. The core innovation lies in the methodology of generating a training corpus that emphasizes the isolation of design form, thereby enabling the AI model to learn and generalize design intent more effectively.

This invention resides at the intersection of artificial intelligence (AI), computer-aided design (CAD), computer graphics, machine learning, and design theory. It specifically addresses the challenges of automating the creation of training datasets for AI models employed in design applications, with a particular focus on maintaining designer control and integrating seamlessly with existing CAD workflows. A core innovation lies in the methodology of generating a training corpus that emphasizes the isolation of design form, thereby enabling the AI model to learn and generalize design intent more effectively.



\subsection{Description of the Related Art}

Traditional design and rendering processes, especially in industrial and product design, are notoriously labor-intensive and time-consuming. Designers typically create 3D models using CAD software, followed by a laborious rendering process involving meticulous adjustments of materials, textures, lighting, and camera angles to produce high-quality visualizations. This iterative process demands specialized skills and significant time investment, often hindering the rapid exploration of design variations.

While existing AI-based design tools have made strides in automating certain aspects of the design process, they often fall short in providing the level of control and precision required by professional designers. Many of these tools rely on pre-trained models trained on generic datasets, which may not adequately capture the specific design principles or hierarchical relationships inherent in a given design. Consequently, the AI-generated outputs may lack the desired level of fidelity to the designer's intent, often exhibiting stylistic inconsistencies or failing to capture the nuances of the original design.

Several patents and publications have explored related aspects of AI training data generation and image processing. For instance, US20190156487A1 describes a method for automated generation of pre-labeled training data for machine learning models, focusing on image segmentation and masking techniques. US20220215145A1 presents a system for machine learning in rapid automatic computer-aided engineering modeling, emphasizing mesh generation for engineering analysis. US11308357B2 discusses training data generation for automated driving systems, utilizing sensor data from vehicles.

However, these existing solutions do not address the unique challenges of AI-assisted design in CAD contexts. They lack the crucial aspect of form isolation through systematic variation, which is essential for preventing overfitting and preserving the integrity of the designer's original form. Furthermore, they do not offer the comprehensive suite of features and functionalities presented in this invention, such as advanced UI controls, location-based background integration, and design analysis feedback.

\section{Problems to be Solved by the Invention}

This invention addresses several critical challenges in AI-assisted design:

\subsection{Automation of Training Data Generation:}
Existing methods for generating training data for AI design models are often manual and time-consuming. This invention automates this process, significantly reducing the effort required and accelerating the design workflow.

\subsection{Form Isolation for AI Learning:}
Current AI models may struggle to learn and generalize design forms effectively due to the confounding influence of stylistic variations and visual attributes. This invention introduces a method for isolating the core design form as the constant element, allowing the AI to focus on learning the form itself.

\subsection{Preventing Overfitting in AI Models}
AI models trained on limited or biased datasets are prone to overfitting, hindering their ability to generalize to new contexts and limiting their creative potential. This invention employs systematic variation of visual attributes to create a diverse training dataset, mitigating the risk of overfitting.

\subsection{Seamless Integration of CAD and AI}
There exists a gap between traditional CAD workflows and AI-assisted design tools, hindering the widespread adoption of AI in professional design practice. This invention bridges this gap by integrating seamlessly with existing CAD software and providing a user-friendly interface for interacting with AI models.

\subsection{Maintaining Designer Control}
Many existing AI-assisted design tools lack the fine-grained control over design outputs that designers require. This invention prioritizes designer control, allowing designers to specify adjustments to CMF properties and other visual attributes while preserving the integrity of the original design form.

\subsection{Enhancing Design Workflow Efficiency}
The traditional design process involves numerous iterative steps, often requiring manual adjustments and revisions. This invention streamlines the design workflow by automating tedious tasks, enabling rapid exploration of design variations, and facilitating seamless collaboration between designers and stakeholders.


\section{Summary of the Invention}

This invention provides a system and method for automating the generation of training data for AI-assisted design applications, focusing on form isolation to prevent overfitting and enhance designer control. The key components and functionalities include:

\subsection{Key Components and Functionalities}

\subsubsection{Object Grouping in CAD Models}
%Leveraging existing CAD functionalities (layers, groups, tags) to define hierarchical object relationships that reflect design intent. This structured organization facilitates semantic understanding and automated label generation. This structure mirrors the way designers think about their creations, breaking down complex forms into meaningful components and sub-systems. The code example provided demonstrates how object groupings (represented by sublayers in Rhino) are used to apply materials to specific parts of the model. This granular control over material application is essential for generating diverse variations and training the AI to understand the relationship between form and material.
Designers utilize existing CAD functionalities, such as layers, groups, or tags, to organize objects within their models according to design principles and hierarchical relationships. This structured organization facilitates semantic understanding of the design components and enables automated labeling of the training data.

\subsubsection{Form Isolation through Systematic Variation}
%Generating diverse variations of the CAD model by systematically altering visual properties (CMF), camera viewpoints, lighting conditions, and backgrounds while maintaining the core design form as the invariant element. This form isolation is the central innovation, enabling the AI model to learn the form independently of other visual attributes and preventing overfitting. The code snippet illustrates this by randomly assigning materials and lighting to the different parts of the chair model, effectively creating numerous variations while the chair's shape remains constant. This process, applied across a wide range of parameters, generates a robust training dataset that emphasizes form recognition.
The system systematically generates a diverse set of variations of the CAD model by altering visual properties (color, texture, material), camera viewpoints (angle, position, focal length), lighting conditions (intensity, direction, color temperature), and backgrounds (color, texture, environment), while meticulously preserving the core design form as the constant element. This form isolation is crucial for preventing overfitting and enabling the AI model to learn the underlying design intent.

\subsubsection{Automated Semantic Label Generation}
%Automatically creating descriptive text labels for each generated variation, capturing object groupings, applied variations, and design intent. These labels provide the textual context necessary for training the AI model and facilitating natural language interaction with the system. The code example shows how these labels are constructed by combining the class name (e.g., "chair"), instance name, viewpoint description, lighting description, and material descriptions. This structured approach ensures consistency and clarity in the labels, allowing the AI to effectively associate text prompts with specific visual attributes.
For each generated variation, the system automatically generates descriptive text labels that capture the essential attributes of the image, including object groupings, applied variations, and design intent. These semantic labels are used to train the AI model and enable natural language interaction with the system.

\subsubsection{AI Model Training using LoRA}
%Fine-tuning a pre-trained text-to-image AI model (e.g., Stable Diffusion) using Low-Rank Adaptation (LoRA). This efficient method adapts the model to the specific design data without retraining the entire model, preserving computational resources and accelerating the training process. LoRA is particularly well-suited for this application as it allows the model to learn the nuances of the specific design while retaining the general image generation capabilities of the pre-trained model.
The generated dataset of images and corresponding semantic labels is used to fine-tune a pre-trained text-to-image AI model, such as Stable Diffusion, using Low-Rank Adaptation (LoRA). This efficient fine-tuning technique allows the model to adapt to the specific design data while preserving its general capabilities.

\subsubsection{AI-Assisted Design Interface with Advanced Features}
%A user-friendly interface enabling designers to interact with the trained AI model through natural language prompts. This interface includes advanced features that streamline the design workflow and enhance creative exploration:
The system provides a user-friendly interface for interacting with the trained AI model. Designers can input natural language prompts to generate new renderings, specifying adjustments to CMF properties and other visual attributes. The interface also includes advanced features such as automated file management, location-based background integration, cost and environmental impact analysis, automated RFQ generation, design callouts, character continuity management, dynamic UI sliders, patent drawing generation, design analysis feedback, client annotation tools, and image security features.

\paragraph{Automated File Management} (Naming, Storage, Archiving): Automates file organization and storage in the cloud, simplifying asset management and facilitating collaboration.

\paragraph{Detailed Close-Ups} Generates high-resolution close-up renders of specific design elements.

\paragraph{Location-Based Backgrounds} (Google Maps Integration, Depth Maps, ControlNet): Incorporates real-world locations as backgrounds, enhancing realism and context.

\paragraph{Mass \& Volume Calculations} (Cost Estimation, Environmental Impact, Automated RFQs): Provides valuable data for design optimization and cost analysis.

\paragraph{Design Callouts} (Automated Generation, Editable Annotations): Automates the creation of professional-style design callouts.

\paragraph{Character Continuity} (Face Swapping, Consistent Descriptions): Ensures consistent character appearance across multiple renders.

\paragraph{Dynamic UI Sliders} (Context-Sensitive Parameters): Provides intuitive control over rendering parameters based on the prompt.

\paragraph{Patent Drawing Generation} Automates the creation of patent drawings from the CAD model.

\paragraph{Design Analysis} (GPT Integration, Potential Problem Highlighting): Offers valuable feedback on design feasibility and potential issues.

\paragraph{Client Annotation Tools} (Collaborative Review and Feedback): Facilitates collaborative review and feedback directly on rendered images.

\paragraph{Image Security} (Watermarking, Serialization): Protects intellectual property and tracks image usage.

\paragraph{Vision Tracking} (Gaze Analysis, Heatmaps): Provides insights into viewer attention patterns.

\paragraph{Modular UI} (Drag-and-Drop, Texture Blending): Offers a flexible and customizable interface for applying materials and styles.

\paragraph{Style References} (Style Transfer, Dynamic Style Parameters): Allows designers to apply different artistic styles to their renders.

\paragraph{Thematic Sliders} (Mood and Style Control): Provides control over the overall mood and aesthetic of the render.

\paragraph{Preset Management} (Saving, Loading, Sharing): Enables efficient reuse of rendering settings and styles.

\paragraph{Export Options} (3D Printing, Foldable Models, etc.): Supports various output formats for different applications.

\paragraph{History Tree and Visual Diff} Tracks design iterations and visually compares variations.

\section{Brief Description of the Drawings}

Figures 1-10 as described previously, potentially adding more diagrams to illustrate specific UI elements and functionalities.
\begin{figure}[h]
    \centering
    \caption{System Overview Diagram}
    \label{fig:system_overview}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{CAD Model with Object Groupings}
    \label{fig:cad_model_groupings}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{Form Isolation through Systematic Variation Flowchart}
    \label{fig:form_isolation_flowchart}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{Semantic Label Generation Diagram}
    \label{fig:semantic_label_generation}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{AI Model Training Process using LoRA}
    \label{fig:ai_model_training_lora}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{AI-Assisted Design Interface Mockup}
    \label{fig:design_interface_mockup}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{Location-Based Background Integration using Google Maps}
    \label{fig:location_based_background}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{Dynamic UI Sliders for Rendering Parameters}
    \label{fig:dynamic_ui_sliders}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{Automated Design Callout Generation}
    \label{fig:automated_design_callout}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{Modular UI for Texture and Style Application}
    \label{fig:modular_ui_texture_style}
\end{figure}


\section{Outline of the Description of the Invention}

\paragraph{Directions for the Detailed Description outline}

This section provides a comprehensive explanation of the invention, elaborating on the functionalities of each component and the overall workflow.  It will reference the provided code examples to illustrate the implementation of specific features and explain how the system addresses the challenges outlined in the previous sections.

\subsection{System Components}
\paragraph{A. CAD System}  
\begin{itemize}
    \item Supported CAD Software:  List specific compatible software (e.g., Rhinoceros 3D, Fusion 360, SolidWorks) and the rationale for their selection (e.g., feature set, scripting capabilities).
    \item Object Grouping Mechanisms: Explain how object groupings are represented in different CAD systems (e.g., layers in Rhino, components in Fusion 360) and how these mechanisms are leveraged by the system.
    \item Importing and Preprocessing CAD Data: Detail the process of importing CAD models into the system and any necessary preprocessing steps (e.g., mesh conversion, geometry simplification).
\end{itemize}
\paragraph{B. Automated Data Generation Module}

B. Automated Data Generation Module
\subparagraph{Modularity}
\begin{itemize}
    \item Identifying Object Groupings: Describe the algorithms and methods used to automatically identify object groupings within the CAD model based on user-defined criteria or pre-defined rules. Reference the code example where sublayers are used to define object groups.
    \item Generating Variations:
\end{itemize}
\subparagraph{Variation}
\begin{itemize}
    \item Visual Properties (CMF): Detail the process of systematically varying color, material, and finish properties. Explain how randomness and predefined ranges are used, referencing the \texttt{MATERIAL\_LIST} and \texttt{create\_random\_material()} function from the code.
    \item Camera Viewpoints: Explain how camera positions, angles, orientations, and focal lengths are varied. Reference the \texttt{generate\_camera\_positions()} and \texttt{generate\_viewpoint\_description()} functions from the code, explaining how hero shot and orbital representation modes are implemented.
    \item Lighting Conditions: Describe the process of generating diverse lighting scenarios, including variations in intensity, direction, color temperature, and type of light source. Reference the \texttt{generate\_lighting\_variation()} and \texttt{apply\_lighting\_to\_render()} functions.
    \item Backgrounds: Explain how different backgrounds are applied to the rendered images, including solid colors, gradients, textures, patterns, and environmental contexts. Discuss the integration with Google Maps for location-based backgrounds, using depth maps and ControlNet.
    \item Semantic Label Generation: Detail the process of automatically generating descriptive text labels for each variation. Explain how the labels incorporate information about object groupings, applied variations, and design intent. Reference the \texttt{generate\_caption()} function and explain how it combines different elements to create a comprehensive caption.
    \item Data Augmentation Techniques: Discuss any additional data augmentation techniques used to further diversify the training dataset (e.g., random cropping, rotation, noise addition).
\end{itemize}


\paragraph{C. AI Model Training Module}      1. Pre-trained Model Selection: Justify the choice of the pre-trained text-to-image AI model (e.g., Stable Diffusion) and explain its advantages in the context of design.
      2. Fine-tuning with LoRA: Detail the process of fine-tuning the pre-trained model using Low-Rank Adaptation (LoRA).  Explain the benefits of LoRA in terms of efficiency and targeted adaptation.
      3. Training Process and Hyperparameter Tuning: Describe the training process, including data preparation, batch size, learning rate, and number of epochs.  Explain how hyperparameters are tuned to optimize model performance.
      4. Evaluation Metrics:  Discuss the metrics used to evaluate the performance of the trained AI model (e.g., image quality, fidelity to design intent, diversity of generated outputs).

\paragraph{D. AI-Assisted Design Interface}
   **D. AI-Assisted Design Interface**

      1.  **Core Rendering Functionality:** Explain how designers use natural language prompts to generate renderings. Discuss the parsing of prompts and the translation of text descriptions into rendering parameters.

      2.  **Advanced Features:**  Provide a detailed explanation of each advanced feature, including implementation details and user interaction:
         a. Automated File Management (Naming, Cloud Storage, Archiving): Discuss the implementation details and benefits of automated file organization, cloud storage integration, and version control.
         b. Detailed Close-Ups: Explain how users can specify regions of interest for generating high-resolution close-ups.
         c. Location-Based Backgrounds: Detail the integration with Google Maps API, depth map generation, and the use of ControlNet for precise background placement and control.
         d. Mass \& Volume Calculations:  Explain how the system calculates mass, volume, estimated cost, and environmental impact based on CMF properties and CAD data.  Describe the automated RFQ generation process.
         e. Design Callouts: Detail the process of automatically generating design callouts using image segmentation and prompt generation.  Explain how users can edit and customize the callouts.
         f. Character Continuity:  Explain the techniques used to maintain character consistency across renders, including face swapping and text-based character description analysis.
         g. Dynamic UI Sliders: Describe how sliders are dynamically generated based on the prompt context and how they provide intuitive control over rendering parameters.
         h. Patent Drawing Generation: Detail the process of automatically generating patent drawings from the CAD data, including line art generation and annotation placement.
         i. Design Analysis:  Explain the integration with a GPT-based model for design analysis.  Describe how potential problems and areas for improvement are identified and presented to the user.
         j. Client Annotation Tools: Describe the collaborative annotation features, allowing clients and reviewers to provide feedback directly on rendered images.
         k. Image Security:  Explain the watermarking and serialization techniques used to protect intellectual property and track image usage.
         l. Vision Tracking: Detail the implementation of eye-tracking analysis and the generation of heatmaps to visualize user attention patterns.
         m. Modular UI:  Describe the drag-and-drop interface, texture blending capabilities, and customization options.
         n. Style References: Explain how users can apply different artistic styles to their renders and how style parameters are controlled.
         o. Thematic Sliders: Discuss the implementation of sliders for controlling mood and overall style.
         p. Preset Management: Detail the preset management system, allowing users to save, load, and share rendering configurations.
         q. Export Options:  List the supported export formats and their respective applications.
         r. History Tree and Visual Diff: Explain how the system tracks design iterations and provides visual comparison capabilities.


\paragraph{II. Process Steps}    

Provide a detailed walkthrough of the entire process, from importing the CAD model to generating final renderings, referencing the specific functionalities of each component described in Section I.

\begin{itemize}
   \item Preparing the CAD Model: Importing the CAD model, defining object groupings, and setting up any necessary preprocessing steps.
   \item Generating the Training Dataset: Running the Automated Data Generation Module to create variations and semantic labels.  Illustrate this with specific examples and reference the code provided.
   \item Training the AI Model:  Fine-tuning the pre-trained model using the generated dataset and LoRA.  Discuss the training parameters and evaluation metrics.
   \item Generating Renderings with the AI-Assisted Design Interface:  Demonstrate the use of natural language prompts and the various advanced features of the interface.  Provide specific examples and illustrate the workflow with screenshots or mockups.
\end{itemize}



\section{Detailed Description of the Invention}

I. System Components

\subsection{I. Object Grouping in CAD Models}

The system is designed to be compatible with a range of industry-standard CAD software, enhancing its accessibility and integration into existing design workflows. While the principles of the invention apply broadly, specific implementations may leverage particular features of different CAD packages. The following describes the supported software and their relevance to the invention:

Supported CAD Software: The system is designed for compatibility with CAD software packages that allow for hierarchical object organization and programmatic access to model data. These features are crucial for automating the variation generation and semantic labeling processes. Specifically, the current implementation supports:

Rhinoceros 3D (Rhino): Rhino is chosen for its robust scripting capabilities using Python, enabling deep integration with the Automated Data Generation Module. The provided code examples demonstrate this integration, utilizing Rhino's Python API (rhinoscriptsyntax) to access and manipulate model data, such as object layers, materials, and rendering settings. Rhino’s widespread use in industrial design and its open architecture make it an ideal platform for developing and deploying this invention.

Autodesk Fusion 360: Fusion 360 is another suitable CAD platform due to its parametric modeling capabilities and API access. Its cloud-based nature facilitates collaborative workflows and integration with other Autodesk services. While not explicitly demonstrated in the provided code, the principles of the invention can be readily applied to Fusion 360 using its Python API.

SolidWorks: SolidWorks, with its extensive use in engineering and product design, is also a target platform for this invention. Its API, while primarily supporting C++, can be accessed through COM interop from Python, enabling similar functionalities as in Rhino and Fusion 360.

The system is not limited to these specific CAD packages. Any CAD software offering similar functionalities in terms of object hierarchy and programmatic access can be integrated with the system. The core principles of the invention remain applicable across different CAD platforms.

Object Grouping Mechanisms: A fundamental aspect of the invention is the utilization of object groupings within the CAD model to represent design intent and hierarchical relationships. Different CAD systems employ various mechanisms for grouping objects, all of which can be leveraged by the system.

\begin{itemize}
    \item Layers (Rhino): In Rhino, layers provide a primary means of organizing objects. The provided code explicitly uses layers (specifically sublayers denoted by "::") to group different parts of the chair model (e.g., "Chair::Legs," "Chair::Seat," "Chair::Backrest"). This layer-based organization directly corresponds to the concept of object groupings in this invention. The code demonstrates how materials are applied to objects based on their layer assignment, highlighting the importance of this hierarchical structure for generating variations.
    \item Components (Fusion 360): Fusion 360 utilizes components as a fundamental organizational structure. Similar to layers in Rhino, components can represent distinct parts or sub-assemblies within a design. The system can be adapted to utilize component information for object grouping and variation generation in Fusion 360.

    \item Groups/Bodies (SolidWorks): SolidWorks employs groups and bodies for organizing geometry. The system can be implemented to recognize these groupings and utilize them for applying variations and generating semantic labels.
\end{itemize}
The system is designed to be adaptable to different object grouping mechanisms. The core concept of organizing objects hierarchically to represent design intent remains consistent, regardless of the specific CAD software or its internal representation of groups.

\paragraph{Importing and Preprocessing CAD Data}
The process of importing CAD models into the system and any necessary preprocessing steps are crucial for ensuring compatibility and efficient data handling.

Direct Import: Ideally, the system directly imports the native file format of the chosen CAD software (e.g., .3dm for Rhino, .f3d for Fusion 360, .sldprt for SolidWorks). This direct import minimizes data loss and preserves the hierarchical structure of the model.

Mesh Conversion (Optional): In some cases, it may be necessary to convert the CAD geometry into a mesh representation for efficient rendering and processing. This conversion can be performed within the CAD software itself or using external libraries. The system handles mesh data appropriately, maintaining the object groupings and applying variations to the mesh elements. If mesh conversion is required, the system will ideally use the highest fidelity mesh representation available, balancing detail with computational efficiency.

Geometry Simplification (Optional): For very complex CAD models, geometry simplification techniques may be employed to reduce the computational load during variation generation and rendering. Simplification techniques, such as mesh decimation or NURBS curve reduction, can be applied while preserving the overall form and object groupings. The system incorporates parameters to control the level of simplification, allowing designers to balance detail with performance. The decision to simplify geometry is context-dependent and depends on the complexity of the model and the computational resources available. The system provides feedback to the designer if simplification is necessary and allows them to adjust the simplification parameters as needed.

\subsection{II. Automated Data Generation Module}

The Automated Data Generation Module is the core of the invention, responsible for creating the diverse and richly labeled dataset required to train the AI model.  It interacts directly with the CAD system, extracting object groupings, applying systematic variations, and generating corresponding semantic labels.

\subparagraph{Identifying Object Groupings:} This module employs algorithms to parse the CAD model and identify object groupings based on user-defined criteria or pre-defined rules.  This process leverages the hierarchical organization mechanisms within the CAD system.

    \begin{itemize}
        \item User-Defined Criteria: Designers can specify criteria for grouping objects, such as shared layer assignments, group memberships, or tag attributes. This allows for flexible and context-specific definition of object groupings that align with the designer's intent.

        \item Pre-defined Rules: For common design patterns, the system incorporates pre-defined rules for identifying object groupings. For instance, in furniture design, rules might be implemented to automatically group objects belonging to "legs," "seat," "backrest," etc., even if these objects are not explicitly grouped by the designer in the CAD model.

    \begin{verbatim}
      \#  Example code for identifying object groupings in Rhino
      The `get_objects_from_sublayers()` function iterates through all sublayers (identified by the "::" delimiter) and extracts the objects belonging to each sublayer. This layer-based grouping provides a direct mapping between the CAD model's organization and the system's understanding of design components.
    \end{verbatim}  

\subparagraph{Generating Variations:}  This module systematically applies variations to the CAD model across four key aspects: visual properties, camera viewpoints, lighting conditions, and backgrounds.

    \begin{itemize}
        \item Visual Properties (CMF): The system varies the color, material, and finish (CMF) properties of objects within the defined groupings.

        \begin{verbatim}
         \# Randomness and Predefined Ranges}
         The code example showcases the use of randomness and predefined ranges for material assignment. The `MATERIAL_LIST` defines a library of materials with their associated properties (name, RGB color, gloss, and reflectivity).  The `create_random_material()` function randomly selects a material from this list and applies it to the objects within a group. This randomized approach ensures a diverse range of CMF variations in the training dataset.  The system also allows for user-defined ranges for each CMF property, providing more control over the variation process.
        \end{verbatim}

        \item Procedural Materials: Beyond the predefined material library, the system can incorporate procedural material generation techniques. This allows for the creation of an infinite number of material variations, further enhancing the diversity of the training data.

        \item Material Blending:  The system can blend multiple materials together, creating complex and nuanced surface appearances.  This allows for more realistic and sophisticated representations of real-world materials.
    \end{itemize}

   \subparagraph{ \item }Camera Viewpoints:  The system varies the camera perspective to capture the design from different angles and distances.

\item
\begin{verbatim}
  \# Hero Shot and Orbital Representation
  Hero Shot and Orbital Representation:** The code provides examples of two camera viewpoint generation modes: "hero shot" and "orbital representation." The `generate_camera_positions()` function implements these modes. "Hero shot" generates camera positions clustered around a focal point, simulating product photography. "Orbital representation" generates camera positions along circular orbits around the object, providing a more comprehensive view of the design. The `variation_degrees` parameter controls the degree of randomness in camera positioning within each mode.
\end{verbatim}

\begin{itemize}
    \item \textbf{Focal Length Variation:} The \texttt{generate\_viewpoint\_description()} function demonstrates the variation of focal length, simulating different lens effects and perspectives. This adds another dimension to the viewpoint variations.
\end{itemize}

\textbf{c. Lighting Conditions:} The system generates diverse lighting scenarios, mimicking different environments and times of day.

\begin{itemize}
    \item \textbf{Lighting Parameters:} The \texttt{generate\_lighting\_variation()} function creates variations in lighting softness, direction, and intensity. This function provides a simplified example; the full system allows for more granular control over lighting parameters, including color temperature, shadows, and the type of light source (e.g., directional, point, spot, area).

    \item \textbf{Applying Lighting to Render:} The \texttt{apply\_lighting\_to\_render()} function (placeholder in the provided code) interacts with the Rhino rendering engine to apply the generated lighting settings. The full implementation utilizes the Rhino API to control various lighting parameters and create realistic lighting effects.
\end{itemize}

\textbf{d. Backgrounds:} The system applies different backgrounds to the rendered images, enhancing context and visual diversity.

\begin{itemize}
    \item \textbf{Background Library:} A library of predefined backgrounds, including solid colors, gradients, textures, patterns, and environmental scenes, is used to create variations.

    \item \textbf{Google Maps Integration:} The system integrates with Google Maps, allowing designers to use real-world locations as backgrounds. It utilizes depth maps derived from Google Maps data and ControlNet to ensure accurate placement and perspective of the 3D model within the chosen environment.
\end{itemize}

\item \textbf{Semantic Label Generation:} For each generated variation, the system creates a detailed semantic label.

\begin{itemize}
    \item \textbf{Structured Description:} The \texttt{generate\_caption()} function in the code provides a basic example of semantic label generation. The full system generates more comprehensive labels, incorporating information about:
    \begin{itemize}
        \item \textbf{Object Groupings and Materials:} "Chair legs made of brushed aluminum, seat upholstered in dark blue fabric, backrest made of light wood."
        \item \textbf{Camera Viewpoint:} "Viewed from a high angle, front right perspective, with a 50mm lens."
        \item \textbf{Lighting Conditions:} "Illuminated with soft, natural light from the left."
        \item \textbf{Background:} "Against a neutral gray background."
        \item \textbf{Design Intent (if provided by the user):} "Designed for a modern office environment."
    \end{itemize}
\end{itemize}

This structured approach ensures consistency and facilitates the AI model's understanding of the relationship between textual descriptions and visual attributes.

\item \textbf{Data Augmentation Techniques:} To maximize the diversity of the training dataset and improve the AI model's robustness, the system employs various data augmentation techniques:

\begin{itemize}
    \item \textbf{Random Cropping:} Randomly cropping sections of the rendered images.
    \item \textbf{Rotation and Flipping:} Rotating and flipping images to create variations in orientation.
    \item \textbf{Color Jitter:} Randomly adjusting brightness, contrast, saturation, and hue.
    \item \textbf{Noise Addition:} Adding small amounts of random noise to the images to simulate real-world imperfections.
\end{itemize}

These techniques artificially expand the training dataset, improving the AI model's ability to generalize and reducing the risk of overfitting. The specific data augmentation techniques used can be customized and adjusted based on the design context and the characteristics of the dataset.

In summary, the systematic variation of visual properties, camera viewpoints, lighting conditions, and backgrounds, combined with the automated generation of semantic labels, forms the foundation for training a robust and versatile AI model for design applications. The referenced code examples illustrate how these functionalities are implemented in practice, demonstrating the practicality and effectiveness of the invention.

\subsection{III. AI Model Training Module}


%This module is responsible for training the AI model that will ultimately empower designers to generate renderings from natural language prompts. It leverages a pre-trained text-to-image model as a foundation and fine-tunes it using the dataset generated by the Automated Data Generation Module. This approach combines the general image generation capabilities of a large pre-trained model with the specific design knowledge encapsulated in the custom dataset.



This module is the heart of the invention, responsible for imbuing the AI with the ability to understand and generate variations of the designer's intended form. Instead of merely learning to associate specific visual styles with a design, the AI is trained to recognize and reproduce the form itself as the primary element, independent of potentially distracting variations in color, material, texture, lighting, or viewpoint. This is achieved by leveraging the meticulously curated dataset generated by the Automated Data Generation Module, which embodies the core principle of form isolation.

\subsubsection{Pre-trained Model Selection}
The system starts with a pre-trained text-to-image AI model, chosen from a range of suitable open-source options. The key criteria for model selection are:

\begin{itemize}
    \item \textbf{Open-Source License:} Essential for maintaining complete ownership and control over the technology, allowing for unrestricted customization, private deployment, and future development.
    \item \textbf{Architectural Flexibility:} The model's architecture should be adaptable for fine-tuning, ideally through efficient techniques like Low-Rank Adaptation (LoRA), which allow for targeted modification without retraining the entire model.
    \item \textbf{Adequate Image Quality:} The model should be capable of generating images with sufficient quality and resolution for design visualization purposes. However, perfect photorealism is not necessarily the primary goal at this stage; the focus is on the model's ability to learn and represent form.
\end{itemize}

Examples of potential open-source models include Stable Diffusion, or other emerging diffusion-based models known for their flexibility and efficiency. The specific choice of model is not critical to the invention; rather, it's the subsequent training process that imbues the model with the unique capability to understand and generate design forms independently of stylistic variations.

\subsubsection{Training for Form Recognition, Not Style Matching}
The heart of this invention lies in how the AI model is trained. Unlike conventional approaches that focus on teaching the AI to mimic specific visual styles or aesthetics, this system trains the AI to recognize and reproduce the underlying form of the design as the primary element. This is achieved by leveraging the unique dataset generated by the Automated Data Generation Module, which explicitly embodies the form isolation principle:

\paragraph{Dataset Structure:} The dataset consists of numerous variations of the same design, where the form remains constant while all other visual attributes (CMF properties, camera viewpoints, lighting, backgrounds) are systematically and randomly varied. This forces the AI to focus on the only consistent element across all images: the form itself.

\paragraph{Semantic Label Guidance:} The semantic labels accompanying each image reinforce this focus on form. Instead of merely describing the specific visual features present in each image, the labels emphasize the underlying form and its constituent parts. For example, a label might read: "A chair with [varied material] legs, a [varied material] seat, and a [varied material] backrest, viewed from a [varied angle]." This structured labeling, combined with the dataset's visual emphasis on form, guides the AI to learn a representation of the form that is decoupled from specific stylistic choices.

\subsubsection{Fine-tuning with LoRA}
Low-Rank Adaptation (LoRA) is a highly efficient fine-tuning technique that allows the pre-trained model to learn from the custom dataset without retraining the entire model from scratch. This is particularly beneficial when working with large, complex models:

\begin{itemize}
    \item \textbf{Targeted Modification:} LoRA injects small, low-rank matrices into specific layers of the pre-trained model, allowing for targeted modifications that specialize the model's behavior without disrupting its general capabilities. This ensures that the model retains its ability to understand and generate images from text prompts, while also incorporating the unique knowledge gained from the form-focused dataset.
    \item \textbf{Preventing Overfitting:} LoRA's efficiency is particularly valuable in this context. Because the training dataset consists of numerous variations of the same form, there is a risk of the model overfitting to the specific variations present in the dataset. LoRA's ability to adapt the model with minimal parameter changes mitigates this risk, ensuring that the model learns the general concept of the form rather than memorizing the specific variations seen during training.
\end{itemize}

\subsubsection{Evaluation and Validation}
The training process is closely monitored and evaluated to ensure that the model is effectively learning to recognize and generate the intended form.

\begin{itemize}
    \item \textbf{Visual Inspection:} Designers play a crucial role in evaluating the model's outputs. They visually inspect the generated images to assess their fidelity to the original form, ensuring that the AI is capturing the essential design elements and relationships.
    \item \textbf{Quantitative Metrics:} Objective metrics like Fréchet Inception Distance (FID) and CLIP Score are used to measure the quality and relevance of the generated images. However, these metrics are interpreted in the context of the invention's specific goal: The focus is not solely on generating photorealistic images, but rather on ensuring that the generated images accurately represent the intended form, regardless of the stylistic variations applied.
    \item \textbf{Generalization Tests:} The model is tested on unseen prompts and variations to assess its ability to generalize beyond the specific examples seen during training. This ensures that the AI can effectively generate novel variations of the form, responding to designer prompts that specify new CMF properties, lighting conditions, or viewpoints, while still preserving the essential design characteristics.
\end{itemize}

This revised description of the AI Model Training Module clarifies the crucial distinction between training for form recognition and conventional style matching. It emphasizes how the form-isolated dataset, combined with targeted fine-tuning using LoRA, enables the AI to learn and generalize the designer's intended form as the primary element, decoupling it from specific stylistic variations. This approach is central to the invention's ability to empower designers with unprecedented control over visual attributes while maintaining the integrity of their original design.

\subsection{IV. AI-Assisted Design Interface}

This component serves as the bridge between the designer's intent and the AI's rendering capabilities. It's a user-friendly interface that empowers designers to interact with the trained AI model through natural language, leveraging its unique ability to generate variations of a design while preserving its core form. The interface goes beyond basic text-to-image functionality, incorporating a suite of advanced features designed to streamline the design workflow, enhance creative exploration, and facilitate collaboration.

\subsubsection{Core Rendering Functionality}

\textbf{Natural Language Prompts}: Designers interact with the AI using natural language descriptions. They can specify desired changes to the CMF properties of the design, adjust lighting conditions, request specific camera viewpoints, and even incorporate elements like characters or backgrounds.

\textbf{Prompt Parsing and Parameter Translation}: The interface parses the designer's text prompt, identifying key elements and translating them into rendering parameters that the AI model can understand. This involves natural language processing techniques to extract relevant information, such as:

\begin{itemize}
    \item \textbf{Object References}: Identifying which parts of the design the prompt refers to (e.g., "the legs," "the seat," "the backrest"). This leverages the object groupings defined in the CAD model.
    \item \textbf{CMF Properties}: Extracting desired changes to color, material, and finish. For example, "make the legs brushed aluminum," "upholster the seat in blue fabric," or "give the backrest a wood grain texture."
    \item \textbf{Lighting and Viewpoint}: Interpreting descriptions of lighting conditions (e.g., "soft natural light," "dramatic spotlight") and camera viewpoints (e.g., "front view," "isometric perspective").
\end{itemize}

\textbf{Rendering Generation}: The AI model, guided by the extracted parameters, generates a rendering of the design, incorporating the specified changes while preserving the integrity of the original form. The model's ability to separate form from style, as trained by the form-isolated dataset, is crucial here. It allows designers to freely manipulate visual attributes without altering the fundamental design.

\subsubsection{2. Advanced Features}

Beyond core rendering functionality, the interface offers a rich set of advanced features to enhance the design process:

\subsubsection{automated File Management}
\begin{itemize}
    \item \textbf{Automated Naming}: The system generates descriptive file names for renderings based on the prompt content, eliminating manual naming and ensuring easy identification. For example, a prompt like "chair with red fabric seat, side view, warm lighting" might result in a file name like \textit{Chair\_RedFabricSeat\_SideView\_WarmLighting.jpg}.
    \item \textbf{Cloud Storage}: Renderings and associated data are automatically stored in the cloud, providing secure access from anywhere and facilitating collaboration. Version control is integrated, allowing designers to track changes and revert to previous versions if needed.
    \item \textbf{Archiving}: Projects and related data can be archived for future reference, ensuring easy retrieval and organization.
\end{itemize}

\subsubsection{Detailed Close-Ups}:
\begin{itemize}
    \item \textbf{Region Selection}: Designers can specify regions of interest within the prompt or directly on the rendered image to generate high-resolution close-ups of specific design elements.
    \item \textbf{Resolution Control}: The system allows for adjustable resolution settings for close-up renders, enabling designers to create highly detailed visualizations for presentations or manufacturing purposes.
\end{itemize}

\subsubsection{Location-Based Backgrounds}:
\begin{itemize}
    \item \textbf{Google Maps Integration}: The system seamlessly integrates with Google Maps, allowing designers to select real-world locations as backgrounds for their renderings. The user can specify a location and viewpoint directly within the prompt (e.g., "place the bench in Central Park, facing the Bethesda Terrace").
    \item \textbf{Depth Map Generation}: The system automatically retrieves depth information from Google Maps data for the chosen location, ensuring accurate placement and perspective of the design within the background.
    \item \textbf{ControlNet}: ControlNet is employed to precisely position the design object within the 3D environment created from the Google Maps data.
\end{itemize}

\subsubsection{Mass \& Volume Calculations}:
\begin{itemize}
    \item \textbf{Automated Analysis}: The system automatically calculates the estimated weight, volume, and material costs of the design.
    \item \textbf{Environmental Impact}: The system estimates the environmental impact of the design based on the chosen materials and manufacturing processes.
    \item \textbf{Automated RFQ Generation}: The system generates requests for quotations (RFQs) based on the calculated material requirements and manufacturing specifications.
\end{itemize}

\subsubsection{Design Callouts}:
\begin{itemize}
    \item \textbf{Automated Callout Generation}: The system can automatically generate professional-style design callouts on the rendered images.
    \item \textbf{Editable Annotations}: Designers can edit the text, adjust the positioning, change the style, and add or remove callouts as needed.
    \item \textbf{Searchable PDF Export}: The callouts are embedded as searchable text elements when exporting the rendering as a PDF.
\end{itemize}

\subsubsection{Character Continuity}:
\begin{itemize}
    \item \textbf{Consistent Character Appearance}: When rendering scenes involving human figures, the system ensures consistent character appearance across multiple variations.
    \item \textbf{Face Swapping}: The system employs face-swapping techniques to maintain a consistent facial appearance for the chosen character, even as other aspects of the scene vary.
    \item \textbf{Text-Based Description Matching}: The system analyzes the text prompts for descriptions of the character and ensures consistency across different renders.
\end{itemize}

\subsubsection{Dynamic UI Sliders}:
\begin{itemize}
    \item \textbf{Context-Sensitive Sliders}: Sliders for controlling rendering parameters are generated based on the text prompt context.
    \item \textbf{Intuitive Control}: These sliders provide an intuitive way to fine-tune the rendering without modifying the text prompt.
    \item \textbf{Parameter Mapping}: The system maps the slider values to specific rendering parameters in the AI model.
\end{itemize}

\subsubsection{Patent Drawing Generation}:
\begin{itemize}
    \item \textbf{Automated Line Art}: The system can automatically generate line art drawings from the CAD model suitable for patent applications.
    \item \textbf{Annotation Placement}: Annotations such as dimension lines and reference numerals are automatically placed according to patent drawing conventions.
    \item \textbf{Export Format Compliance}: The generated patent drawings are exported in a format compliant with the specific requirements of the target patent office.
\end{itemize}

\subsubsection{Design Analysis}:
\begin{itemize}
    \item \textbf{GPT Integration}: The system integrates with a large language model (LLM) for design analysis.
    \item \textbf{Problem Highlighting}: The LLM flags potential problems and provides feedback to the designer.
    \item \textbf{Contextual Feedback}: The analysis takes into account the specific context described in the prompt.
\end{itemize}

\subsubsection{Client Annotation Tools}:
\begin{itemize}
    \item \textbf{Collaborative Review}: Tools for clients to add annotations and comments directly on the rendered images.
    \item \textbf{Visual Feedback}: Clients can draw on the image, highlight areas, or use pre-defined stamps.
    \item \textbf{Revision Tracking}: All annotations and comments are tracked, facilitating iterative revisions.
\end{itemize}

\subsubsection{Image Security}:
\begin{itemize}
    \item \textbf{Watermarking}: Invisible watermarks are embedded within generated images to protect intellectual property.
    \item \textbf{Serialization}: Each rendered image is assigned a unique serial number for tracking usage and distribution.
    \item \textbf{Usage Monitoring}: The system detects unauthorized use or appearance of watermarked images online.
\end{itemize}

\subsubsection{Vision Tracking & form review}:
\begin{itemize}
    \item \textbf{Eye-Tracking Integration}: Eye-tracking technology, using the embedded webcam on a laptop or rear-facing camera on a mobile phone, as an example, analyzes user gaze patterns during the review process.
    \item \textbf{Heatmap Visualization}: Eye-tracking data is visualized as heatmaps overlaid on the rendered images for the designer to review, indicating what part of the design is calling the attention of the viewer. The degree to which the time and temporal order in which this happens and how much is corrisponds to the explicit hierarchy of the design elements may be quantified, with surely many industrial designers will appreciate as a method of reviewing the success of their work. It allows one answer to the question, "are people noticing the part of my design I want them to notice, or are they looking at something else". Such review allows the designer to potentially make changes to porportion, repeition and contrast in their compositions on the CMF or form level of the design.
\end{itemize}

\subsubsection{Modular UI}:
\begin{itemize}
    \item \textbf{Drag-and-Drop Interface}: Designers can drag and drop images or textures onto the design to apply materials or patterns.
    \item \textbf{Texture Blending}: The system supports blending multiple textures to create complex material effects.
\end{itemize}

\subsubsection{Style References}:
\begin{itemize}
    \item \textbf{Style Transfer}: Designers can apply artistic styles to renderings by providing reference images.
    \item \textbf{Dynamic Style Parameters}: Sliders control the strength and intensity of the applied style.
\end{itemize}

\subsubsection{Thematic Sliders}:
\begin{itemize}
    \item \textbf{Mood and Atmosphere}: Sliders control the overall mood and atmosphere of the rendering.
    \item \textbf{Style Presets}: Thematic sliders can be combined into presets for predefined stylistic looks
\subsubsection{itemize}
\item \textbf{Style Presets}: Thematic sliders can be combined into presets for predefined stylistic looks (e.g., “cinematic,” “retro,” “minimalist”).
\end{itemize}

\subsubsection{Preset Management}:
\begin{itemize}
\item \textbf{Saving and Loading Presets}: Designers can save their preferred rendering configurations as presets, including CMF choices, lighting settings, camera viewpoints, style references, and thematic slider settings.
\item \textbf{Sharing Presets}: Presets can be shared among team members or publicly, fostering consistency and collaboration in design visualization.
\end{itemize}

\subsubsection{Export Options}:
\begin{itemize}
\item \textbf{Image Formats}: The system supports exporting renders in various image formats, including JPG, PNG, TIFF, and EXR, catering to different resolution and quality requirements.
\item \textbf{3D Printable Models}: Designers can export the design as a 3D printable model, allowing for rapid prototyping and physical evaluation.
\item \textbf{Foldable Paper Models}: The system can generate patterns for creating foldable paper models of the design, providing a tangible and interactive way to explore the form.
\item \textbf{Vector Graphics}: Designers can export the rendering as vector graphics (e.g., SVG), enabling scalability and integration with other design software.
\end{itemize}

\subsubsection{History Tree and Visual Diff}:
\begin{itemize}
\item \textbf{Iteration Tracking}: The interface maintains a history of all design iterations, including changes to the prompt, applied materials, lighting settings, and camera viewpoints. This provides a clear record of the design process and allows designers to easily revert to previous versions.
\item \textbf{Visual Diff}: The system highlights the differences between two selected versions of the rendering, visually showing the changes 



\section{Claims}

\subsection{TO DO:}
Expand claims to thoroughly cover the described features and functionalities, including the core innovation of form isolation and its application to preventing overfitting.)

\section{Conclusion}

This invention significantly advances the field of AI-assisted design by addressing key challenges in training data generation, workflow integration, and designer control. By focusing on form isolation and incorporating a comprehensive suite of advanced features, the system empowers designers to leverage the creative potential of AI while maintaining full control over their design intent and streamlining their workflow. The provided code examples offer a glimpse into the technical implementation of the system, demonstrating how the core functionalities are realized in practice. 

\end{document}